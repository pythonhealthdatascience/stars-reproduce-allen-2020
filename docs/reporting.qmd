---
title: "Reporting"
---

This page evaluates the extent to which Monks et al. 2016 meets the criteria from two discrete-event simulation study reporting guidelines:

* STRESS-DES: Strengthening The Reporting of Empirical Simulation Studies (Discrete-Event
Simulation) (Version 1.0)
* The generic reporting checklist for healthcare-related discrete event simulation studies derived from the the International Society for Pharmacoeconomics and Outcomes Research Society for Medical Decision Making (ISPOR-SDM) Modeling Good Research Practices Task Force reports.

## STRESS-DES

<mark>example fill in, not actual</mark>

| Section/Item | Recommendation | Met by Monks et al. 2016? |
| - | -- | - |
| **Objectives** | 
| 1.1 Purpose of the model | Explain the background and objectives for the model | ‚úÖ Fully |
| 1.2 Model outputs | Define all quantitative performance measures that are reported, using equations where necessary.  Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated. | üü° Partially |
| 1.3 Experimentation aims | If the model has been used for experimentation, state the objectives that it was used to investigate.<br>(A) Scenario based analysis ‚Äì Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.<br>(B) Design of experiments ‚Äì Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).<br>(C) Simulation Optimisation ‚Äì (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used.  Where possible provide a citation of the algorithm(s). | ‚ùå Not met |
| **Logic** |
| 2.1 Base model overview diagram | Describe the base model using appropriate diagrams and description.  This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers.  Avoid complicated diagrams in the main text.  The goal is to describe the breadth and depth of the model with respect to the system being studied. | ‚ùî To evaluate |
| 2.2 Base model logic | Give details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works. | ‚ùî To evaluate |
| 2.3 Scenario logic | Give details of the logical difference between the base case model and scenarios (if any).  This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2. | ‚ùî To evaluate |
| 2.4 Algorithms | Provide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e.  scheduling of arrivals/appointments/operations/maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible.  Pseudo-code may be used to describe an algorithm. | ‚ùî To evaluate |
| 2.5.1 Components - entities | Give details of all entities within the simulation including a description of their role in the model and a description of all their attributes. | ‚ùî To evaluate |
| 2.5.2 Components - activities | Describe the activities that entities engage in within the model.  Provide details of entity routing into and out of the activity. | ‚ùî To evaluate |
| 2.5.3 Components - resources | List all the resources included within the model and which activities make use of them. | ‚ùî To evaluate |
| 2.5.4 Components - queues | Give details of the assumed queuing discipline used in the model (e.g. First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each.  If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues. | ‚ùî To evaluate |
| 2.5.5 Components - entry/exit points | Give details of the model boundaries i.e. all arrival and exit points of entities.  Detail the arrival mechanism (e.g. ‚Äòthinning‚Äô to mimic a non-homogenous Poisson process or balking) | ‚ùî To evaluate |
| **Data** |
| 3.1 Data sources | List and detail all data sources. Sources may include:<br>‚Ä¢ Interviews with stakeholders,<br>‚Ä¢ Samples of routinely collected data,<br>‚Ä¢ Prospectively collected samples for the purpose of the simulation study,<br>‚Ä¢ Public domain data published in either academic or organisational literature.   Provide, where possible, the link and DOI to the data or reference to published literature.<br>All data source descriptions should include details of the sample size, sample date ranges and use within the study. | ‚ùî To evaluate |
| 3.2 Pre-processing | Provide details of any data manipulation that has taken place before its use in the simulation, e.g. interpolation to account for missing data or the removal of outliers. | ‚ùî To evaluate |
| 3.3 Input parameters | List all input variables in the model. Provide a description of their use and include parameter values.  For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters.  Give details of all time dependent parameters and correlation.<br>Clearly state:<br>‚Ä¢ Base case data<br>‚Ä¢ Data use in experimentation, where different from the base case.<br>‚Ä¢ Where optimisation or design of experiments has been used, state the range of values that parameters can take.<br>Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions. | ‚ùî To evaluate |
| 3.4 Assumptions | Where data or knowledge of the real system is unavailable what assumptions are included in the model?  This might include parameter values, distributions or routing logic within the model. | ‚ùî To evaluate |
| **Experimentation** |
| 4.1 Initialisation | Report if the system modelled is terminating or non-terminating.  State if a warm-up period has been used, its length and the analysis method used to select it.  For terminating systems state the stopping condition.<br>State what if any initial model conditions have been included, e.g., pre-loaded queues and activities.  Report whether initialisation of these variables is deterministic or stochastic. | ‚ùî To evaluate |
| 4.2 Run length | Detail the run length of the simulation model and time units. | ‚ùî To evaluate |
| 4.3 Estimation approach | State the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for   the methods used and the number of replications/size of batches. | ‚ùî To evaluate |
| **Implementation** |
| 5.1 Software or programming language | State the operating system and version and build number.<br>State the name, version and build number of commercial or open source DES software that the model is implemented in.<br>State the name and version of general-purpose programming languages used (e.g. Python 3.5).<br>Where frameworks and libraries have been used provide all details including version numbers. | ‚ùî To evaluate |
| 5.2 Random sampling | State the algorithm used to generate random samples in the software/programming language used e.g. Mersenne Twister.<br>If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes. | ‚ùî To evaluate |
| 5.3 Model execution | State the event processing mechanism used e.g. three phase, event, activity, process interaction.<br>*Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.*<br>State all priority rules included if entities/activities compete for resources.<br>If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used.  For parallel and distributed simulations the time management algorithms used.  If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.) | ‚ùî To evaluate |
| 5.4 System specification | State the model run time and specification of hardware used.  This is particularly important for large scale models that require substantial computing power.  For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.) | ‚ùî To evaluate |
| **Code access** |
| 6.1 Computer model sharing statement | Describe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results.  Provide, where possible, the link and DOIs to these. | ‚ùî To evaluate |

: Compliance with STRESS-DES reporting guidelines

<mark>are the components all to inc. seperate? presume so</mark>

<mark>if we apply this later down the line, what about if its not discrete-event?</mark>

<mark>do we just want to say if it's met, or do we want to populate it like Tom did in the simpy example? presume just the former?</mark>

Could include emojis to help with reading through

‚úÖüü¢üåü‚≠êüéØü•áüòäüôÇ

üü°ü•à‚ûñ„Ä∞Ô∏èüòê

‚ùåüî¥üõëüï≥Ô∏è‚ùóüôÅ

## ISPOR-SDM