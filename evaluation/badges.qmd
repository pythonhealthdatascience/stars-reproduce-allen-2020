---
title: "Badges"
format:
  html:
    code-fold: true
jupyter: python3
---

<!--
Template for evaluation of journal article against criteria of various reproducibility badges.

To use this template:
1. For each criteria in the Python cell, add a 0 if not met and 1 if it is met.
2. Run the script (i.e. preview the Quarto book) - you will see that the code outputs a formatted list of criteria met, as well as the badges met.

Although this script uses Python, it is applicable regardless of the language used by the study you are evaluating.
-->

This page evaluates the extent to which the author-published research artefacts meet the criteria of badges related to reproducibility from various organisations and journals.

## Code to evaluate against criteria for each badge

```{python}
import numpy as np
import pandas as pd

# Criteria and their definitions
criteria = {
    'archive': 'Stored in a permanent archive that is publicly and openly accessible',
    'id': 'Has a persistent identifier',
    'license': 'Includes an open license',
    'complete': 'Complete set of materials shared (as would be needed to fully reproduce article)',
    'documentation_sufficient': 'Artefacts are sufficiently documented (eg. to understand how it works, to enable it to be run, including package versions)',
    'documentation_careful': 'Artefacts are carefully documented (more than sufficient - e.g. to the extent that reuse is facilitated)',
    'relevant': '''Arefacts are relevant to and contribute to the article's results''',
    'execute': 'Scripts can be successfully executed',
    'structure': 'Artefacts are well structured/organised (e.g. to the extent that reuse is facilitated, adhering to norms and standards of research community)',
    'regenerated': 'Independent party regenerated results using the authors research artefacts',
    'hour': 'Reproduced within approximately one hour (excluding compute time)',
    # This criteria is kept seperate to documentation_careful, as it specifically requires a README file
    'documentation_readme': 'Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript'
}

# Full badge names
badge_names = {
    # Open objects
    'open_niso': 'NISO "Open Research Objects (ORO)"',
    'open_niso_all': 'NISO "Open Research Objects - All (ORO-A)"',
    'open_acm': 'ACM "Artifacts Available"',
    'open_cos': 'COS "Open Code"',
    'open_ieee': 'IEEE "Code Available"',
    # Object review
    'review_acm_functional': 'ACM "Artifacts Evaluated - Functional"',
    'review_acm_reusable': 'ACM "Artifacts Evaluated - Reusable"',
    'review_ieee': 'IEEE "Code Reviewed"',
    # Results reproduced
    'reproduce_niso': 'NISO "Results Reproduced (ROR-R)"',
    'reproduce_acm': 'ACM "Results Reproduced"',
    'reproduce_ieee': 'IEEE "Code Reproducible"',
    'reproduce_psy': 'Psychological Science "Computational Reproducibility"'
}

# Criteria required by each badge
badges = {
    # Open objects
    'open_niso': ['archive', 'id', 'license'],
    'open_niso_all': ['archive', 'id', 'license', 'complete'],
    'open_acm': ['archive', 'id'],
    'open_cos': ['archive', 'id', 'license', 'complete', 'documentation_sufficient'],
    'open_ieee': ['complete'],
    # Object review
    'review_acm_functional': ['documentation_sufficient', 'relevant', 'complete', 'execute'],
    'review_acm_reusable': ['documentation_sufficient', 'documentation_careful', 'relevant', 'complete', 'execute', 'structure'],
    'review_ieee': ['complete', 'execute'],
    # Results reproduced
    'reproduce_niso': ['regenerated'],
    'reproduce_acm': ['regenerated'],
    'reproduce_ieee': ['regenerated'],
    'reproduce_psy': ['regenerated', 'hour', 'structure', 'documentation_readme'],
}

# Evaluation for this study
eval = pd.Series({
    'archive': 1,
    'id': 1,
    'license': 1,
    'complete': 1,
    'documentation_sufficient': 1,
    'documentation_careful': 0,
    'relevant': 1,
    'execute': 1,
    'structure': 1,
    'regenerated': 1,
    'hour': 0,
    'documentation_readme': 0,
})

# Identify which badges would be awarded based on criteria
award = {}
for badge in badges:
    award[badge] = all([eval[key] == 1 for key in badges[badge]])
```

## Simple presentation of results

```{python}
# Print index
print('''Key:
‚úÖ = Meets criteria
‚¨ú = Does not meet criteria
üìù = Not yet evaluated''')

# Print compliance to each criteria
print('\nCriteria:')
for key, value in eval.items(): 
    if value == 1:
        icon = '‚úÖ'
    elif value == 0:
        icon = '‚¨ú'
    else:
        icon = 'üìù'
    print(f'{icon} {criteria[key]}')

# Print results
print('\nBadges:')
for key, value in award.items(): 
    if value:
        icon = '‚úÖ'
    else:
        icon = '‚¨ú'
    print(f'{icon} {badge_names[key]}')

award_list = list(award.values())
print(f'\nMet criteria for {sum(award_list)} of {len(award_list)} badges ({round(sum(award_list)/len(award_list)*100, 1)}%)')
```

## More detailed presentation of results

In total, the original study met the criteria for **10 of the 12** badges. This included:

* **5 of the 5** "open objects" badges
* **2 of the 3** "object review" badges
* **2 of the 3** "reproduced" badges

### "Open objects" badges

These badges relate to research artefacts being made openly available.

::: {.callout-tip appearance="minimal" collapse=true}

## ‚úÖ NISO "Open Research Objects (ORO)"

Meets all criteria:

* ‚úÖ Stored in a permanent archive that is publicly and openly accessible
* ‚úÖ Has a persistent identifier
* ‚úÖ Includes an open license
:::

::: {.callout-warning appearance="minimal" collapse=true}

## ‚¨ú ACM "Artifacts Evaluated - Reusable"

Does not meet all criteria:

* ‚úÖ Stored in a permanent archive that is publicly and openly accessible
* ‚úÖ Has a persistent identifier
* ‚úÖ Includes an open license
* ‚úÖ Complete set of materials shared (as would be needed to fully reproduce article)
* ‚úÖ Artefacts are sufficiently documented (eg. to understand how it works, to enable it to be run, including package versions)
* ‚¨ú Artefacts are carefully documented (more than sufficient - e.g. to the extent that reuse is facilitated)
:::

### Full criteria list

Between each badge, there is lots of overlapping criteria. Below is a list of whether the original study met each of the unique criteria items.

* ‚úÖ Stored in a permanent archive that is publicly and openly accessible
* ‚úÖ Has a persistent identifier
* ‚úÖ Includes an open license
* ‚úÖ Complete set of materials shared (as would be needed to fully reproduce article)
* ‚úÖ Artefacts are sufficiently documented (eg. to understand how it works, to enable it to be run, including package versions)
* ‚¨ú Artefacts are carefully documented (more than sufficient - e.g. to the extent that reuse is facilitated)
* ‚úÖ Arefacts are relevant to and contribute to the article's results
* ‚úÖ Scripts can be successfully executed
* ‚úÖ Artefacts are well structured/organised (e.g. to the extent that reuse is facilitated, adhering to norms and standards of research community)
* ‚úÖ Independent party regenerated results using the authors research artefacts
* ‚¨ú Reproduced within approximately one hour (excluding compute time)
* ‚¨ú Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript