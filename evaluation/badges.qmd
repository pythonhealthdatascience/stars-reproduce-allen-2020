---
title: "Badges"
format:
  html:
    code-fold: true
jupyter: python3
---

<!--
Template for evaluation of journal article against criteria of various reproducibility badges.

To use this template:
1. For each criteria in the Python cell, add a 0 if not met and 1 if it is met.
2. Run the script (i.e. preview the Quarto book) - you will see that the code outputs a formatted list of criteria met, as well as the badges met.

Although this script uses Python, it is applicable regardless of the language used by the study you are evaluating.
-->

This page evaluates the extent to which the author-published research artefacts meet the criteria of badges related to reproducibility from various organisations and journals.

## Criteria

```{python}
from IPython.display import display, Markdown
import numpy as np
import pandas as pd

# Criteria and their definitions
criteria = {
    'archive': 'Stored in a permanent archive that is publicly and openly accessible',
    'id': 'Has a persistent identifier',
    'license': 'Includes an open license',
    'complete': 'Complete set of materials shared (as would be needed to fully reproduce article)',
    'documentation_sufficient': 'Artefacts are sufficiently documented (eg. to understand how it works, to enable it to be run, including package versions)',
    'documentation_careful': 'Artefacts are carefully documented (more than sufficient - e.g. to the extent that reuse is facilitated)',
    'relevant': '''Arefacts are relevant to and contribute to the article's results''',
    'execute': 'Scripts can be successfully executed',
    'structure': 'Artefacts are well structured/organised (e.g. to the extent that reuse is facilitated, adhering to norms and standards of research community)',
    'regenerated': 'Independent party regenerated results using the authors research artefacts',
    'hour': 'Reproduced within approximately one hour (excluding compute time)',
    # This criteria is kept seperate to documentation_careful, as it specifically requires a README file
    'documentation_readme': 'Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript'
}

# Full badge names
badge_names = {
    # Open objects
    'open_niso': 'NISO "Open Research Objects (ORO)"',
    'open_niso_all': 'NISO "Open Research Objects - All (ORO-A)"',
    'open_acm': 'ACM "Artifacts Available"',
    'open_cos': 'COS "Open Code"',
    'open_ieee': 'IEEE "Code Available"',
    # Object review
    'review_acm_functional': 'ACM "Artifacts Evaluated - Functional"',
    'review_acm_reusable': 'ACM "Artifacts Evaluated - Reusable"',
    'review_ieee': 'IEEE "Code Reviewed"',
    # Results reproduced
    'reproduce_niso': 'NISO "Results Reproduced (ROR-R)"',
    'reproduce_acm': 'ACM "Results Reproduced"',
    'reproduce_ieee': 'IEEE "Code Reproducible"',
    'reproduce_psy': 'Psychological Science "Computational Reproducibility"'
}

# Criteria required by each badge
badges = {
    # Open objects
    'open_niso': ['archive', 'id', 'license'],
    'open_niso_all': ['archive', 'id', 'license', 'complete'],
    'open_acm': ['archive', 'id'],
    'open_cos': ['archive', 'id', 'license', 'complete', 'documentation_sufficient'],
    'open_ieee': ['complete'],
    # Object review
    'review_acm_functional': ['documentation_sufficient', 'relevant', 'complete', 'execute'],
    'review_acm_reusable': ['documentation_sufficient', 'documentation_careful', 'relevant', 'complete', 'execute', 'structure'],
    'review_ieee': ['complete', 'execute'],
    # Results reproduced
    'reproduce_niso': ['regenerated'],
    'reproduce_acm': ['regenerated'],
    'reproduce_ieee': ['regenerated'],
    'reproduce_psy': ['regenerated', 'hour', 'structure', 'documentation_readme'],
}

# Evaluation for this study
eval = pd.Series({
    'archive': 1,
    'id': 1,
    'license': 1,
    'complete': 1,
    'documentation_sufficient': 1,
    'documentation_careful': 0,
    'relevant': 1,
    'execute': 1,
    'structure': 1,
    'regenerated': 1,
    'hour': 0,
    'documentation_readme': 0,
})

# Identify which badges would be awarded based on criteria
award = {}
for badge in badges:
    award[badge] = all([eval[key] == 1 for key in badges[badge]])

# Get list of badges met (True/False) overall
award_list = list(award.values())

# Create Markdown list with...
callout_icon = {True: '✅',
                False: '⬜'}
full_criteria_list = ''.join([
    '* ' +
    callout_icon[eval[key]] + # Icon based on whether it met criteria
    ' ' +
    value + # Full text description of criteria
    '\n' for key, value in criteria.items()])

# Create text section
display(Markdown(f'''
To assess whether the author's materials met the requirements of each badge, a list of criteria was produced. Between each badge (and between categories of badge), there is often alot of overlap in criteria. This study met **{sum(award_list)} of the {len(award_list)}** unique criteria items. These were as follows (where '✅' indicates criteria were met, whilst '⬜' indicates they were not):

{full_criteria_list}'''))
```

## Badges
```{python}

# Write introduction
# Get list of badges met (True/False) by category
award_open = [v for k,v in award.items() if k.startswith('open_')]
award_review = [v for k,v in award.items() if k.startswith('review_')]
award_reproduce = [v for k,v in award.items() if k.startswith('reproduce_')]

# Create and display text for introduction
display(Markdown(f'''
In total, the original study met the criteria for **{sum(award_list)} of the {len(award_list)} badges**. This included:

* **{sum(award_open)} of the {len(award_open)}** “open objects” badges
* **{sum(award_review)} of the {len(award_review)}** “object review” badges
* **{sum(award_reproduce)} of the {len(award_reproduce)}** “reproduced” badges
'''))

# Make function that creates collapsible callouts for each badge
def create_badge_callout(award_dict):
    '''
    Displays Markdown callouts created for each badge in the dictionary, showing
    whether the criteria for that badge was met.

    Parameters:
    -----------
    award_dict : dict
        Dictionary where key is badge (as variable name), and value is Boolean
        (whether badge is awarded)
    '''
    callout_appearance = {True: 'tip',
                          False: 'warning'}
    callout_icon = {True: '✅',
                    False: '⬜'}
    callout_text = {True: 'Meets all criteria:',
                    False: 'Does not meet all criteria:'}

    for key, value in award_dict.items():
        # Create Markdown list with...
        criteria_list = ''.join([
            '* ' +
            callout_icon[eval[k]] + # Icon based on whether it met criteria
            ' ' +
            criteria[k] + # Full text description of criteria
            '\n' for k in badges[key]])
        # Create the callout and display it
        display(Markdown(f'''
::: {{.callout-{callout_appearance[value]} appearance="minimal" collapse=true}}

## {callout_icon[value]} {badge_names[key]}

{callout_text[value]}

{criteria_list}
:::
'''))

# Create badge functions with introductions and callouts
display(Markdown('''
### "Open objects" badges

These badges relate to research artefacts being made openly available.
'''))
create_badge_callout({k: v for (k, v) in award.items() if k.startswith('open_')})

display(Markdown('''
### "Object review" badges

These badges relate to the research artefacts being reviewed against criteria of the badge issuer.
'''))
create_badge_callout({k: v for (k, v) in award.items() if k.startswith('review_')})

display(Markdown('''
### "Reproduced" badges

These badges relate to an independent party regenerating the reuslts of the article using the author objects.
'''))
create_badge_callout({k: v for (k, v) in award.items() if k.startswith('reproduce_')})
```
