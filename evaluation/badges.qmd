---
title: "Badges"
format:
  html:
    code-fold: true
jupyter: python3
---

<!--
Template for evaluation of journal article against criteria of various reproducibility badges.

To use this template:
1. For each criteria in the Python cell, add a 0 if not met and 1 if it is met.
2. Run the script (i.e. preview the Quarto book) - you will see that the code outputs a formatted list of criteria met, as well as the badges met.

Although this script uses Python, it is applicable regardless of the language used by the study you are evaluating.
-->

This page evaluates the extent to which the author-published research artefacts meet the criteria of badges related to reproducibility from various organisations and journals.

```{python}
import numpy as np
import pandas as pd

criteria = {
    'archive': 'Stored in a permanent archive that is publicly and openly accessible',
    'id': 'Has a persistent identifier',
    'license': 'Includes an open license',
    'complete': 'Complete set of materials shared (as would be needed to fully reproduce article)',
    'documentation_sufficient': 'Artefacts are sufficiently documented (eg. to understand how it works, to enable it to be run, including package versions)',
    'documentation_careful': 'Artefacts are carefully documented (more than sufficient - e.g. to the extent that reuse is facilitated)',
    'relevant': '''Arefacts are relevant to and contribute to the article's results''',
    'execute': 'Scripts can be successfully executed',
    'structure': 'Artefacts are well structured/organised (e.g. to the extent that reuse is facilitated, adhering to norms and standards of research community)',
    'regenerated': 'Independent party regenerated results using the authors research artefacts',
    'hour': 'Reproduced within approximately one hour (excluding compute time)',
    # This criteria is kept seperate to documentation_careful, as it specifically requires a README file
    'documentation_readme': 'Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript'
}

badge_names = {
    # Open objects
    'open_niso': 'NISO "Open Research Objects (ORO)"',
    'open_niso_all': 'NISO "Open Research Objects - All (ORO-A)"',
    'open_acm': 'ACM "Artifacts Available"',
    'open_cos': 'COS "Open Code"',
    'open_ieee': 'IEEE "Code Available"',
    # Object review
    'review_acm_functional': 'ACM "Artifacts Evaluated - Functional"',
    'review_acm_reusable': 'ACM "Artifacts Evaluated - Reusable"',
    'review_ieee': 'IEEE "Code Reviewed"',
    # Results reproduced
    'reproduce_niso': 'NISO "Results Reproduced (ROR-R)"',
    'reproduce_acm': 'ACM "Results Reproduced"',
    'reproduce_ieee': 'IEEE "Code Reproducible"',
    'reproduce_psy': 'Psychological Science "Computational Reproducibility"'
}

badges = {
    # Open objects
    'open_niso': ['archive', 'id', 'license'],
    'open_niso_all': ['archive', 'id', 'license', 'complete'],
    'open_acm': ['archive', 'id'],
    'open_cos': ['archive', 'id', 'license', 'complete', 'documentation_sufficient'],
    'open_ieee': ['complete'],
    # Object review
    'review_acm_functional': ['documentation_sufficient', 'relevant', 'complete', 'execute'],
    'review_acm_reusable': ['documentation_sufficient', 'documentation_careful', 'relevant', 'complete', 'execute', 'structure'],
    'review_ieee': ['complete', 'execute'],
    # Results reproduced
    'reproduce_niso': ['regenerated'],
    'reproduce_acm': ['regenerated'],
    'reproduce_ieee': ['regenerated'],
    'reproduce_psy': ['regenerated', 'hour', 'structure', 'documentation_readme'],
}
```

```{python}
# Example (not done for Allen et al. 2020 yet)
# Based on the criteria dictionary above, populate with 1 and 0
eval = pd.Series({
    'archive': 1,
    'id': 1,
    'license': 1,
    'complete': 1,
    'documentation_sufficient': 1,
    'documentation_careful': 0,
    'relevant': 1,
    'execute': 1,
    'structure': 1,
    'regenerated': 1,
    'hour': 0,
    'documentation_readme': 0,
})
```

‚úÖ = Meets criteria ‚¨ú = Does not meet criteria üìù = Not yet evaluated

```{python}
# Print compliance to each criteria
for key, value in eval.items(): 
    if value == 1:
        icon = '‚úÖ'
    elif value == 0:
        icon = '‚¨ú'
    else:
        icon = 'üìù'
    print(f'{icon} {criteria[key]}')
```

```{python}
# Identify which badges would be awarded based on criteria
award = {}
for badge in badges:
    award[badge] = all([eval[key] == 1 for key in badges[badge]])

# Print results
for key, value in award.items(): 
    if value:
        icon = '‚úÖ'
    else:
        icon = '‚¨ú'
    print(f'{icon} {badge_names[key]}')

award_list = list(award.values())
print(f'Met criteria for {sum(award_list)} of {len(award_list)} badges ({round(sum(award_list)/len(award_list)*100, 1)}%)')
```
