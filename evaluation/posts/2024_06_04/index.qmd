---
title: "Day 5"
author: "Amy Heather"
date: "2024-06-04"
categories: [guidelines]
---

::: {.callout-note}

Evaluated study against guidelines (badges, sharing artefacts, and starting on STRESS-DES reporting).

:::

## Work log

### Badges

Evaluating artefacts as in <https://zenodo.org/records/3760626> (as has been copied into `original_study/`).

Felt uncertain around these criteria:

* `documentation_sufficient` - it was missing one package from environment - but it had an environment file, package versions, and a README explaining how to set up environment and referring to a notebook which runs the code that produces the output - hence, despite missing one, I felt it met this criteria
* `documentation_careful` - I feel the documentation is minimal, but that it was actually still sufficient for running the model - and, as such, presume it might meet this criteria? Unless "reuse" refers to being able to use and change - in which case, there is not much guidance on how to change the parameters, only on how to run it matching up with the paper? In which case, I don't think it meets these. I think that interpretation also makes sense, as it then distinguishes from "sufficient"?
    * The criteria this relates to is from @association_for_computing_machinery_acm_artifact_2020 : "The artifacts associated with the paper are of a quality that significantly exceeds minimal functionality. That is, they have all the qualities of the Artifacts Evaluated â€“ Functional level, but, in addition, they are very carefully documented and well-structured to the extent that reuse and repurposing is facilitated. In particular, norms and standards of the research community for artifacts of this type are strictly adhered to"
* `execute` - yes, but with one change to environment. Do they allow changes? I've selected yes, as I'm assuming minor troubleshooting is allowed, and the distinction between this and reproduction is that this is just about running scripts, whilst reproduction is about getting sufficiently similar results. Execution required in ACM and IEEE criteria...
    * From @association_for_computing_machinery_acm_artifact_2020: "Included scripts and/or software used to generate the results in the associated paper can be successfully executed, and included data can be accessed and appropriately manipulated", with no mention of whether minor troubleshooting is allowed
    * From @institute_of_electrical_and_electronics_engineers_ieee_about_nodate: "runs to produce the outputs desired", with no mention of whether minor troubleshooting is allowed
* `regenerated` - as above, unclear whether modification is allowed. For simplicity, have assumed definition we allowed - that you can troubleshoot - but this might not align with journals. Is this an issue?
* `hour` - failed this, but we weren't trying to do this within an hour. If I had been asked to do that (rather than spend time reading and thinking beforehand), I anticipate I could've run it (without going on to then add seeds etc). Hence, does it pass this?
    * Psychological Science (@hardwicke_transparency_2023, @association_for_psychological_science_aps_psychological_2023) require reproduction within an hour, which I think implies some minor troubleshooting would be allowed?
    * Others don't specify either way
* `documentation_readme` - I wouldn't say it explicitly meets this criteria, although it was simple enough that it could do it anyway

### Sharing research artefacts

Evaluated against recommendations for sharing of research artefacts (STARS, and Monks and Harper best practice audit).

Uncertainities:

* Readme for audit - does it matter if that information is in the readme file? can they pass if readme links to template which overviews it? I think so? Although Table 1 in Monks and Harper 2023 specifies item as Readme, the description is just whether there is an obvious file overviewing purpose
* Citation information - not provided for this study, although Zenodo does provide it, but as code materials themselves don't contain it, feel it fails this
* Remote code repository - STARS talks about why you should use them (e.g. version control) - but here, evaluating, someone might have done a single upload and not used properly - but purpose of STARS is not to evaluate if something is right or wrong upload so doesn't specify further - in this case, assuming simplest route of if it is on something like GitHub then it meets criteria
* Enhanced documentation - lots of suggestions of what to do for this
* Remote execution/Online coding environment - Zenodo doesn't but GitLab does. I was largely working with Zenodo as that was the archived version, but in this case, did say met criteria via GitLab. Importance of which code is chosen when there are two versions. I didn't initially think it met criteria until checked the GitLab too. Have recommended both are referred to. But do they meet criteria if changes are made to repository after publication? After submission? For example, if we asked them to add a license.

::: {.callout-tip}
## Keeping code repository and archive in sync and up-to-date

Evaluation was made harder as there were slight differences between the archived code (Zenodo) and the production code (GitLab). You might not always want to keep them in sync. But it would be good to keep them in sync up until submission.
:::

## Reporting guidelines

These were evaluated based ONLY on the journal article and supplementary materials (and not on the linked code repository).

STRESS uncertanties:

* Model outputs - "Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated" - I find this criteria quite hard to apply? Here, it's just N patients and additional time during model, mediane and extreme. Does it need more detail than that to meet it though?
* Experiementation aims - is it applicable? The DES model, as I understand, just tests one scenario - a worst case scenario - so it's not really "scenario based analysis" comparing scenarios - but it does have A scenario and justification for it?
* Algorithms - is this applicable in this example? Parameters are just from sampling from distributions, or proportions. Assuming its applicable, and that distribing those distributions answers this question?
* Queues - wasn't certain about this, as I don't really know what order they get allocated to units. but that's not a queue? but it kind of is - in that, at the start of the day, they are waiting to be assigned?
* Entry/exit points - have said not met as can't find this stated, but also, not sure what the answer is. They are in model on days they need dialysis, and then leave again? unless get unallocated? and then they never return if their state turns to dead?

::: {.callout-tip}
## Including checklist

Tom mentioned how this study they provided the filled out STRESS checklist to the journal, but it wasn't published alongside the article, but that would have been beneficial. This is a good suggestion to think about.

Also, it has proven quite time consuming to evaluate against the checklist - hence, moreso reason to include it with the article, to make it easier for readers to spot these things.
:::

## Suggested changes for protocol/template

âœ… = Made the change.

ðŸ’¬ = Noted to discuss with team.

Protocol: 

* âœ… Suggest that uncertainties on whether a badge criteria is met or not could be discussed within the STARS team. Suggest to detail those uncertainties within logbook
* âœ… If there are multiple code locations (e.g. code repository and archive), then refer to both when assessing against criteria
* âœ… Recommended sources for evaluation (e.g. reporting guidelines is based on article, badges and sharing of research artefacts are based on code)
* ðŸ’¬ Given how long it is taking, would it be relevant to time these steps?