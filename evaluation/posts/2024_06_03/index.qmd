---
title: "Day 4"
author: "Amy Heather"
date: "2024-06-03"
categories: [reproduce]
---

::: {.callout-note}

Found seed that produces results fairly visually similar to paper. Total time used: TBCh TBCm (TBC%)

:::

## Work log

### 10.30-11.03 Modifying code to provide more control over replication seeds

Made changes to `sim_replicate.py`:

* Changed `multiple_replications()` to accept a base number, so the random number set is the replication number plus the base number
* Changed `run_replications()` to accept base number to function, and input this to call of `multiple_replications()`

Started changing `end_trial_analysis.py` to enable the function to accept changes to the filename, but then decided a simpler solution (that doesn't require changing their code in multiple places) is just to use `os.rename()` within our notebook. Set up notebook to run through 15 different base seeds. Paused timing whilst this ran...

### 11.29-11.36 Fixed issue in duplication between my replications

Looked over images, but all looked very similar to one another. Realised this was because my method for modifying the seeds still had lots of matching seeds between each run - for example:

* Base 0: Random number sets are 0-30
* Base 1: Random number sets are 1-31
* Base 2: Random number sets are 2-32

Hence, changed this in `reproduction.ipynb` so the base numbers were 0, 100, 200, 300 etc, and so:

* Base 0: Random number set 0-30
* Base 100: Random number set 100-130
* Base 200: Random number set 200-230

Then paused timing as reran again...

### 11.56-12.00 Compared against article figures

Compared against paper figures. Figures 2 and 3 always generally look quite consistent. Figure 4 can look quite different, and from the runs I've done, you can spot for example that our peaks in the left figure, and in the maximum of the right figure, often look lower.

Trying again with another set of 15, this time 1500 to 2900.

An alternative to this would be to identify the number of replications required for stable results - but that feels like less of a focus on reproducibility, and more of a focus on valid results.

::: {.callout-tip}
## Simulation stability

From 15 different versions, I'm finding the peaks of the figures from the paper to be slightly higher than typical. Assuming that model parameters are correct and that control of randomness is now implemented appropriately, this highlights the relevance of simulation stability. Although 30 replications were performed, there was not assessment of whether this was the number needed for stability of results. This might be relevant to consider for STARS framework.

As in 'Simulation: The Practice of Model Development and Use' by Stewart Robinson, there are three approaches for selecting the number of replications:

* Rule of thumb - Law and McComas (1990) suggest at least 3-5, although models will often need more than that
* Graphical method - Plot cumulative mean of an output in a graph (X axis is replication number, and Y axis is cumulative mean), and identify when that line becomes flat
* Confidence interval method (recommended method) - Similar to above, but including confidence intervals in the plot (e.g. 95%), and basing decision on narrowness of these itnervals. This can be done by calculating the percentage deviations of the confidence interval on each side of the mean with the increasing number of replications. You then set a threshold of the amount of deviation desired.
:::

### ??-

## Timings

```{python}
import sys
sys.path.append('../')
from timings import calculate_times

# Minutes used prior to today
used_to_date = 345

# Times from today
times = [
    ('10.30', '11.03'),
    ('11.29', '11.36')]

# Print time used and remaining
calculate_times(used_to_date, times)
```

## Suggested changes for protocol/template

âœ… = Made the change.

Research compendium:

* Testing - could test whether results are stable with the chosen number of replications? Although that might be too much about validity of results, than about reproducibility of results?